import streamlit as st
import requests
import csv
import time
import logging
import re
from datetime import datetime
import pytz
import os
import pandas as pd

# è¨­ç½® logging åŸºæœ¬é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# é é¢åç¨±èˆ‡å°æ‡‰çš„é•·æœŸå­˜å–æ¬Šæ–å­—å…¸
FACEBOOK_LONG_TERM_TOKEN_DICT = {
    "Vannise": "EAAHlt6ZBwJk8BO6jpnUYPuLTkYXxFZCnWy9fttLmIsPd1S1MtVgyR9zmeZBNQzFwrL4zF8TdxMHc3u8ClDxoaXO1P7IZBZCIh95X5jvum2162ksYYqvaYTdYeEkvBmoQL0acBtN4HHfH82ZA2ZCwaxWa5QZASUTLGYNZBUEzckzex2Lth1tZADRE7la8oeyZAy9dUic",
    "Vanniseä¸æ–·é›»": "EAAHlt6ZBwJk8BO3uxEcRtHTBs73Lq2mwfAZCuSlJ8furlyCwICWpQltijcV75jqqCNA02C3Ub5r4sxQTJworTZC5G1xilqZAxg52Vkl8GmSwBsZBlb9D0r4AQjK5Gj0LktEvDWyvCEqfYriqaCnpEm87rAcu4xL6D8Iz5qn9ZBHTpZCvLj2AnBlvCUkQpKALHI8kQQ5vPzBZBt4ZBgGvYKzJF85Fk"
}

# Streamlit ä»‹é¢è¨­å®š
st.set_page_config(page_title="Facebook Token Selector & Comment Fetcher", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Facebook Token Selector & Comment Fetcher")

# æ–°å¢ä¸‹æ‹‰å¼é¸å–®è®“ä½¿ç”¨è€…é¸æ“‡é é¢åç¨±
selected_page = st.selectbox(
    "è«‹é¸æ“‡ Facebook é é¢åç¨±",
    options=list(FACEBOOK_LONG_TERM_TOKEN_DICT.keys())
)

# æ ¹æ“šä½¿ç”¨è€…é¸æ“‡çš„é é¢åç¨±ï¼Œå¾å­—å…¸ä¸­å–å‡ºå°æ‡‰çš„ token
access_token = FACEBOOK_LONG_TERM_TOKEN_DICT[selected_page]

# é¡¯ç¤ºé¸æ“‡çš„é é¢åç¨±å’Œå°æ‡‰çš„ access token
st.write(f"é¸æ“‡çš„é é¢åç¨±: {selected_page}")
st.write(f"å°æ‡‰çš„ Access Token: {access_token}")

# ç²å–ç•¶å¤©æ—¥æœŸä¸¦æ ¼å¼åŒ–ç‚º YYYY-MM-DD
today = datetime.now().strftime("%Y-%m-%d")

# ä½¿ç”¨è€…è¼¸å…¥ï¼Œæª”æ¡ˆåç¨±é è¨­ç‚ºç•¶å¤©æ—¥æœŸ
STREAM_NAME = st.text_input("è«‹è¼¸å…¥æª”æ¡ˆåç¨± (ä¾‹å¦‚: 2024-10-16-ä¸æ–·é›».æ—©):", f"{today}-ä¸æ–·é›»")
VIDEO_ID = st.text_input("è«‹è¼¸å…¥ Facebook å½±ç‰‡ ID:", "1543222116619760")

# å‹•æ…‹è¨­å®šæª”æ¡ˆåç¨±
filename = f"{STREAM_NAME}-ç•™è¨€.csv"

# æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨å‰‡å»ºç«‹æª”æ¡ˆ
if not os.path.exists(filename):
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['created_time', 'user_name', 'user_id', 'message', 'comment_id', 'item_id'])  # CSV header
    logging.info(f"æª”æ¡ˆ {filename} ä¸å­˜åœ¨ï¼Œå·²è‡ªå‹•å»ºç«‹")

def extract_item_id(message):
    if "+1" in message:
        match = re.search(r'(\d{4})', message)
        if match:
            return match.group(1)
    return ""

def convert_to_taiwan_time(utc_time_str):
    utc_time = datetime.strptime(utc_time_str, "%Y-%m-%dT%H:%M:%S%z")
    taiwan_tz = pytz.timezone("Asia/Taipei")
    taiwan_time = utc_time.astimezone(taiwan_tz)
    return taiwan_time.strftime("%Y-%m-%d %H:%M:%S")

def fetch_comments(api_url, filename):
    page_count = 0
    with open(filename, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        
        while api_url:
            try:
                response = requests.get(api_url)
                if response.status_code != 200:
                    logging.error(f"Error fetching data: {response.status_code}")
                    break
                
                data = response.json()
                
                page_comments = []
                for comment in data.get('data', []):
                    created_time = comment.get('created_time')
                    message = comment.get('message')
                    user_name = comment.get('from', {}).get('name', 'Unknown')
                    user_id = comment.get('from', {}).get('id', 'Unknown')
                    comment_id = comment.get('id')

                    # å°‡ UTC æ™‚é–“è½‰æ›ç‚ºå°ç£æ™‚é–“
                    taiwan_time = convert_to_taiwan_time(created_time)

                    # æå– item_id
                    item_id = extract_item_id(message)

                    page_comments.append([taiwan_time, user_name, user_id, message, comment_id, item_id])
                
                writer.writerows(page_comments)
                
                page_count += 1
                logging.info(f"æŠ“å–åˆ°ç¬¬ {page_count} é ï¼Œå·²å¯«å…¥ {len(page_comments)} æ¢ç•™è¨€")
                
                api_url = data.get('paging', {}).get('next')

                time.sleep(1)
            
            except Exception as e:
                logging.error(f"An error occurred: {e}")
                break

# åœ¨ Streamlit ä¸­åŸ·è¡Œç•™è¨€æŠ“å–åŠŸèƒ½
if st.button("é–‹å§‹æŠ“å–ç•™è¨€"):
    if not access_token or not VIDEO_ID:
        st.error("è«‹ç¢ºä¿å·²é¸æ“‡é é¢å’Œè¼¸å…¥å½±ç‰‡ ID")
    else:
        st.info("é–‹å§‹æŠ“å–ç•™è¨€ï¼Œè«‹ç¨ç­‰...")
        initial_url = f"https://graph.facebook.com/v20.0/{VIDEO_ID}/comments?limit=1000&access_token={access_token}"
        fetch_comments(initial_url, filename)
        st.success(f"ç•™è¨€å·²æˆåŠŸæŠ“å–ä¸¦å„²å­˜è‡³ {filename}")

        # è®€å– CSV ä¸¦é¡¯ç¤º
        df = pd.read_csv(filename)
        st.dataframe(df)

        # æä¾›ä¸‹è¼‰æŒ‰éˆ•
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ä¸‹è¼‰ç•™è¨€ CSV",
            data=csv,
            file_name=filename,
            mime='text/csv',
        )
